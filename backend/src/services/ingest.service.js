import fs from 'fs/promises';
import { v4 as uuidv4 } from 'uuid';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { qdrant } from '../vectorstore/qdrant.js';
import { extractText } from '../utils/extractText.js';
import dotenv from 'dotenv';
import path from 'path';
dotenv.config();

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

// ‚úÖ Gemini embedding size
const GEMINI_EMBED_DIM = 768;

function cleanText(text) {
  return text
    .replace(/[\x00-\x1F\x7F]/g, '')
    .replace(/\s+/g, ' ')
    .trim();
}

async function ensureCollection(collectionName) {
  try {
    const collections = await qdrant.getCollections();
    if (!collections.collections.find(c => c.name === collectionName)) {
      console.log(`üÜï Creating Qdrant collection: ${collectionName}`);
      await qdrant.createCollection(collectionName, {
        vectors: {
          size: GEMINI_EMBED_DIM, // ‚úÖ always 768 for Gemini
          distance: 'Cosine',
        },
      });
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  } catch (err) {
    if (err.statusCode === 409) {
      console.log(`‚ö†Ô∏è Collection ${collectionName} already exists`);
    } else {
      console.error('‚ùå Failed to check/create collection:', err);
      throw err;
    }
  }
}

async function uploadInBatches(collectionName, points, batchSize = 100) {
  for (let i = 0; i < points.length; i += batchSize) {
    const batch = points.slice(i, i + batchSize);
    await qdrant.upsert(collectionName, { points: batch });
    console.log(`üì§ Uploaded batch ${i / batchSize + 1} (${batch.length} points)`);
  }
}

export async function ingestFile(file, country) {
  console.log(`üì• Ingesting file: ${file.originalname} for country: ${country}`);

  // üëá Expect extractText to return [{ text, page }]
  const rawChunks = await extractText(file.path, file.mimetype);

  const seenChunks = new Set();
  const paragraphs = rawChunks
    .map(({ text, page }) => ({
      text: cleanText(text),
      page,
    }))
    .filter(p => p.text !== '' && !seenChunks.has(p.text) && seenChunks.add(p.text));

  console.log(`üìÑ Cleaned to ${paragraphs.length} unique, non-empty paragraphs`);

  const collection = `legal_chunks_${country.toLowerCase()}-gm`;
  await ensureCollection(collection);

  console.log(`üîÑ Preparing to upload ${paragraphs.length} chunks to collection: ${collection}`);

  const embeddingModel = genAI.getGenerativeModel({ model: 'embedding-001' });
  const points = [];

  for (let i = 0; i < paragraphs.length; i++) {
  let { text: chunk, page } = paragraphs[i];

  if (chunk.length > 3000) {
    console.warn(`‚ö†Ô∏è Chunk ${i} too long (${chunk.length} chars). Truncating.`);
    chunk = chunk.slice(0, 3000);
  }

  try {
    const result = await embeddingModel.embedContent({
      content: { parts: [{ text: chunk }] },
    });

    const vector = result.embedding.values;

    points.push({
      id: uuidv4(),
      vector,
      payload: {
        text: chunk,
        country,
        source: path.basename(file.path), // ‚úÖ fixed
        page,                             // ‚úÖ fixed
      },
    });
  } catch (err) {
    console.error(`‚ö†Ô∏è Embedding failed for chunk ${i}: ${err.message}`);
  }
}


  if (points.length) {
    await uploadInBatches(collection, points);
    console.log(`‚úÖ Uploaded ${points.length} chunks to ${collection}`);
  } else {
    console.warn(`‚ö†Ô∏è No valid chunks found in ${file.originalname}`);
  }

  console.log(`üóëÔ∏è Cleaning up temporary file: ${file.path}`);
  await fs.unlink(file.path);
}
